  #activation function
  1. ReLU
      max(x,0)
      hidden layer used only
      def relu(X):
        return torch.max(input=X, other=torch.tensor(0.0))
  2. Sigmoid
      range of 0 to 1
      the last layer to classfication
  3. tanh
      range of -1 to 1
      the last layer to classfication, easy to solve
 
 
 #model two-layers neural network
 
  1. initialize parameters
  num_inputs, num_outputs, num_hiddens = 784, 10, 256

  W1 = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens)), dtype=torch.float)
  b1 = torch.zeros(num_hiddens, dtype=torch.float)
  W2 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_outputs)), dtype=torch.float)
  b2 = torch.zeros(num_outputs, dtype=torch.float)

  params = [W1, b1, W2, b2]
  for param in params:
      param.requires_grad_(requires_grad=True)
      
      
  2. define net
  def net(X):
    X = X.view((-1, num_inputs))
    H = relu(torch.matmul(X, W1) + b1)
    return torch.matmul(H, W2) + b2
  
  3. forward and backward to update parameters
  
 #pytorch
 net = nn.Sequential(
        d2l.FlattenLayer(),
        nn.Linear(num_inputs, num_hiddens),
        nn.ReLU(),
        nn.Linear(num_hiddens, num_outputs), 
        )
loss = torch.nn.CrossEntropyLoss()

optimizer = torch.optim.SGD(net.parameters(), lr=0.5)

num_epochs = 5
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)
