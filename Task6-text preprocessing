##文本预处理
文本是一类序列数据，预处理包括四个步骤：
1. 读入文本
2. 分词
3. 建立字典
4. 文本从词的序列转为索引的序列


#分词，根据空格分词；根据字符分词；
def tokenize(sentences, token='word'):
    """Split sentences into word or char tokens"""
    if token == 'word':
        return [sentence.split(' ') for sentence in sentences]
    elif token == 'char':
        return [list(sentence) for sentence in sentences]
    else:
        print('ERROR: unkown token type '+token)

tokens = tokenize(lines)
tokens[0:2]

#建立字典
将每个词对应一个索引编号。
class Vocab(object):
    #构造函数
    tokens可以看做是分词的结果
    min_preq是阈值，小于这个阈值的词，忽略
    #use_构造词典时候需要使用特殊的token
    def __init__(self, tokens, min_freq=0, use_special_tokens=False):
        counter = count_corpus(tokens) #统计词频，根据词频筛选词。 
        self.token_freqs = list(counter.items())#根据词频构造频率列表
        self.idx_to_token = []#记录需要用的token
        if use_special_tokens: #添加特殊的token
            # padding, begin of sentence, end of sentence, unknown
            self.pad, self.bos, self.eos, self.unk = (0, 1, 2, 3)
            self.idx_to_token += ['<pad>', '<bos>', 'eos', 'unk']#在比较短的句子补上一个token和最长的句子一样长。bos是句子开头。
        else: #如果为假，
            self.unk = 0
            self.idx_to_token += ['<unk>']
        #统计词频并根据词频筛选，该列表自此需要维护
        self.idx_to_token += [token for token, freq in self.token_freqs
                        if freq >= min_freq and token not in self.idx_to_token]
        self.token_to_idx = dict()
        #根据enumerate遍历
        for idx, token in enumerate(self.idx_to_token):
            self.token_to_idx[token] = idx

    def __len__(self):#定义len函数能够返回字典的大小
        return len(self.idx_to_token)

    def __getitem__(self, tokens):#
        if not isinstance(tokens, (list, tuple)):#如果不是列表和元组
            return self.token_to_idx.get(tokens, self.unk)#如果在其中则返回
        return [self.__getitem__(token) for token in tokens]#如果是列表，则遍历返回

    def to_tokens(self, indices):#给定索引返回对应的词
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]
        return [self.idx_to_token[index] for index in indices]

def count_corpus(sentences):
    tokens = [tk for st in sentences for tk in st]#展平得到一个一维列表
    return collections.Counter(tokens)  # 返回一个字典，记录每个词的出现次数
