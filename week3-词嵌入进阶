词嵌入进阶
在“Word2Vec的实现”一节中，我们在小规模数据集上训练了一个 Word2Vec 词嵌入模型，并通过词向量的余弦相似度搜索近义词。虽然 Word2Vec 已经能够成功地将离散的单词转换为连续的词向量，并能一定程度上地保存词与词之间的近似关系，但 Word2Vec 模型仍不是完美的，它还可以被进一步地改进：
#加入了单词分析语义。
子词嵌入（subword embedding）：FastText 以固定大小的 n-gram 形式将单词更细致地表示为了子词的集合，
而 BPE (byte pair encoding) 算法则能根据语料库的统计信息，自动且动态地生成高频子词的集合；
GloVe 全局向量的词嵌入: 通过等价转换 Word2Vec 模型的条件概率公式，我们可以得到一个全局的损失函数表达，
并在此基础上进一步优化模型。

GloVe 全局向量的词嵌入¶
GloVe 模型
先简单回顾以下 Word2Vec 的损失函数（以 Skip-Gram 模型为例，不考虑负采样近似）：

−∑t=1T∑−m≤j≤m,j≠0logP(w(t+j)∣w(t))
−∑t=1T∑−m≤j≤m,j≠0log⁡P(w(t+j)∣w(t))
 
其中

P(wj∣wi)=exp(u⊤jvi)∑k∈Vexp(u⊤kvi)
P(wj∣wi)=exp⁡(uj⊤vi)∑k∈Vexp⁡(uk⊤vi)
 
是  wiwi  为中心词， wjwj  为背景词时 Skip-Gram 模型所假设的条件概率计算公式，我们将其简写为  qijqij 。

注意到此时我们的损失函数中包含两个求和符号，它们分别枚举了语料库中的每个中心词和其对应的每个背景词。实际上我们还可以采用另一种计数方式，那就是直接枚举每个词分别作为中心词和背景词的情况：

−∑i∈V∑j∈Vxijlogqij
−∑i∈V∑j∈Vxijlog⁡qij
 
其中  xijxij  表示整个数据集中  wjwj  作为  wiwi  的背景词的次数总和。

我们还可以将该式进一步地改写为交叉熵 (cross-entropy) 的形式如下：

−∑i∈Vxi∑j∈Vpijlogqij
−∑i∈Vxi∑j∈Vpijlog⁡qij
 
其中  xixi  是  wiwi  的背景词窗大小总和， pij=xij/xipij=xij/xi  是  wjwj  在  wiwi  的背景词窗中所占的比例。

这里可以看出，我们的词嵌入方法实际上就是想让模型学出  wjwj  有多大概率是  wiwi  的背景词，而真实的标签则是语料库上的统计数据。同时，语料库中的每个词根据  xixi  的不同，在损失函数中所占的比重也不同。

注意到目前为止，我们只是改写了 Skip-Gram 模型损失函数的表面形式，还没有对模型做任何实质上的改动。而在 Word2Vec 之后提出的 GloVe 模型，则是在之前的基础上做出了以下几点改动：

使用非概率分布的变量  p′ij=xijpij′=xij  和  q'ij=exp(u⊤jvi)q′ij=exp⁡(uj⊤vi) ，并对它们取对数；
为每个词  wiwi  增加两个标量模型参数：中心词偏差项  bibi  和背景词偏差项  cici ，松弛了概率定义中的规范性；
将每个损失项的权重  xixi  替换成函数  h(xij)h(xij) ，权重函数  h(x)h(x)  是值域在  [0,1][0,1]  上的单调递增函数，松弛了中心词重要性与  xixi  线性相关的隐含假设；
用平方损失函数替代了交叉熵损失函数。
综上，我们获得了 GloVe 模型的损失函数表达式：

∑i∈V∑j∈Vh(xij)(u⊤jvi+bi+cj−logxij)2


载入时，cache是加载数据集，下载好后，可以通过这个指定压缩文件。



uj的转置*vi，在梯度下降的过程，中心词的词向量与背景词靠近，所以背景词可以决定中心词，而同义词，大多背景词非常相似。

#近义词
def knn(W, x, k):
    '''
    @params:
        W: 所有向量的集合
        x: 给定向量
        k: 查询的数量
    @outputs:
        topk: 余弦相似性最大k个的下标
        [...]: 余弦相似度
    '''
    cos = torch.matmul(W, x.view((-1,))) / (
        (torch.sum(W * W, dim=1) + 1e-9).sqrt() * torch.sum(x * x).sqrt())
    _, topk = torch.topk(cos, k=k)#找出前k大
    topk = topk.cpu().numpy()
    return topk, [cos[i].item() for i in topk]

def get_similar_tokens(query_token, k, embed):
    '''
    @params:
        query_token: 给定的单词
        k: 所需近义词的个数
        embed: 预训练词向量
    '''
    topk, cos = knn(embed.vectors,
                    embed.vectors[embed.stoi[query_token]], k+1)#k+1，排除最相近的那个，即排除自己
    for i, c in zip(topk[1:], cos[1:]):  # 除去输入词
        print('cosine sim=%.3f: %s' % (c, (embed.itos[i])))

get_similar_tokens('chip', 3, glove)



def get_analogy(token_a, token_b, token_c, embed):
    '''
    @params:
        token_a: 词a
        token_b: 词b
        token_c: 词c
        embed: 预训练词向量
    @outputs:
        res: 类比词d
    '''
    vecs = [embed.vectors[embed.stoi[t]] 
                for t in [token_a, token_b, token_c]]
    x = vecs[1] - vecs[0] + vecs[2] #c+b-a
    topk, cos = knn(embed.vectors, x, 1)
    res = embed.itos[topk[0]]
    return res

get_analogy('man', 'woman', 'son', glove)
